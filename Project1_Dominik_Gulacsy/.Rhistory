}
# Create urls for iterative scraping
urls=c()
for (page in 1:pages){
url=paste0(d,'/search?page=',page,'&',kwq)
urls <- urls %>% append(url)
}
print(urls)
df_list_urls <- lapply(urls, scrape_page)
return(rbindlist(df_list_urls))
}
result<-scrape_data_verge("linux",4)
result<-scrape_data_verge("linux",3)
result<-scrape_data_verge("linux",5)
result<-scrape_data_verge("zoom",3)
result<-scrape_data_verge("ubuntu",3)
result<-scrape_data_verge("ubuntu",3)
result<-scrape_data_verge("ubuntu",2)
result<-scrape_data_verge("ubuntu",1)
urls
scrape_page("https://www.theverge.com/search?page=1&q=ubuntu")
p<-read_html("https://www.theverge.com/search?page=1&q=ubuntu")
write_html(p,'p.html')
# Get container divs
containers <-
p %>% html_nodes('.c-compact-river__entry')
# Iterate through the containers and extract html tags
list_of_df <-
lapply(containers, function(container){
title <-
container %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_text()
article_link <-
container %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_attr("href")
bylines <-
container %>%
html_nodes('.c-byline__item')
author <-
container %>%
html_nodes('.c-byline__author-name') %>%
html_text()
author_link <-
bylines[1] %>%
html_nodes('a') %>%
html_attr("href")
dt <-
bylines[2] %>%
html_nodes('time') %>%
html_attr('datetime')
comments <-
container %>%
html_nodes('.c-entry-stat__comment-data') %>%
nth(1) %>%
html_text() %>%
trimws()
if (length(comments) != 0) {
comments <- as.numeric(comments)
comments_link <-
container %>%
html_nodes('.c-entry-stat--words') %>%
html_nodes('a') %>%
html_attr("href")
# OR
# comments_link <- paste0(article_link,'#comments')
} else {
comments <- ''
comments_link <- ''
}
# it will be a one row data.frame
return(data.frame('date'=dt, 'title'=title, 'article_link'= article_link,'author'=author, 'author_link'=author_link, 'comments'=comments, 'comments_link'=comments_link))
})
# Iterate through the containers and extract html tags
list_of_df <-
lapply(containers, function(container){
title <-
container %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_text()
article_link <-
container %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_attr("href")
bylines <-
container %>%
html_nodes('.c-byline__item')
author <-
container %>%
html_nodes('.c-byline__author-name') %>%
html_text()
author_link <-
bylines[1] %>%
html_nodes('a') %>%
html_attr("href")
dt <-
bylines[2] %>%
html_nodes('time') %>%
html_attr('datetime')
comments <-
container %>%
html_nodes('.c-entry-stat__comment-data') %>%
nth(1) %>%
html_text() %>%
trimws()
if (length(comments) != 0) {
comments <- as.numeric(comments)
comments_link <-
container %>%
html_nodes('.c-entry-stat--words') %>%
html_nodes('a') %>%
html_attr("href")
# OR
# comments_link <- paste0(article_link,'#comments')
} else {
comments <- ''
comments_link <- ''
}
# it will be a one row data.frame
return(data.frame('date'=dt, 'title'=title, 'article_link'= article_link,'author'=author, 'author_link'=author_link, 'comments'=comments, 'comments_link'=comments_link))
})
p<-read_html("https://www.theverge.com/search?page=1&q=ubuntu")
write_html(p,'p.html')
# Get container divs
containers <-
p %>% html_nodes('.c-compact-river__entry')
# Iterate through the containers and extract html tags
list_of_df <-
lapply(containers, function(container){
title <-
container %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_text()
article_link <-
container %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_attr("href")
bylines <-
container %>%
html_nodes('.c-byline__item')
author <-
container %>%
html_nodes('.c-byline__author-name') %>%
html_text()
author_link <-
bylines[1] %>%
html_nodes('a') %>%
html_attr("href")
dt <-
bylines[2] %>%
html_nodes('time') %>%
html_attr('datetime')
comments <-
container %>%
html_nodes('.c-entry-stat__comment-data') %>%
nth(1) %>%
html_text() %>%
trimws()
if (length(comments) != 0) {
comments <- as.numeric(comments)
comments_link <-
container %>%
html_nodes('.c-entry-stat--words') %>%
html_nodes('a') %>%
html_attr("href")
# OR
# comments_link <- paste0(article_link,'#comments')
} else {
comments <- ''
comments_link <- ''
}
print(container)
# it will be a one row data.frame
return(data.frame('date'=dt, 'title'=title, 'article_link'= article_link,'author'=author, 'author_link'=author_link, 'comments'=comments, 'comments_link'=comments_link))
})
c=0
# Iterate through the containers and extract html tags
list_of_df <-
lapply(containers, function(container){
title <-
container %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_text()
article_link <-
container %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_attr("href")
bylines <-
container %>%
html_nodes('.c-byline__item')
author <-
container %>%
html_nodes('.c-byline__author-name') %>%
html_text()
author_link <-
bylines[1] %>%
html_nodes('a') %>%
html_attr("href")
dt <-
bylines[2] %>%
html_nodes('time') %>%
html_attr('datetime')
comments <-
container %>%
html_nodes('.c-entry-stat__comment-data') %>%
nth(1) %>%
html_text() %>%
trimws()
if (length(comments) != 0) {
comments <- as.numeric(comments)
comments_link <-
container %>%
html_nodes('.c-entry-stat--words') %>%
html_nodes('a') %>%
html_attr("href")
# OR
# comments_link <- paste0(article_link,'#comments')
} else {
comments <- ''
comments_link <- ''
}
c=c+1
print(c)
# it will be a one row data.frame
return(data.frame('date'=dt, 'title'=title, 'article_link'= article_link,'author'=author, 'author_link'=author_link, 'comments'=comments, 'comments_link'=comments_link))
})
container<- containers[5]
title <-
container %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_text()
title
article_link <-
container %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_attr("href")
article_link
bylines
bylines <-
container %>%
html_nodes('.c-byline__item')
bylines
author <-
container %>%
html_nodes('.c-byline__author-name') %>%
html_text()
author
author_link <-
bylines[1] %>%
html_nodes('a') %>%
html_attr("href")
author_link
length(author_link)
# Iterate through the containers and extract html tags
list_of_df <-
lapply(containers, function(container){
title <-
container %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_text()
article_link <-
container %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_attr("href")
bylines <-
container %>%
html_nodes('.c-byline__item')
author <-
container %>%
html_nodes('.c-byline__author-name') %>%
html_text()
if (length(author) ==0) {
author <- ''
}
author_link <-
bylines[1] %>%
html_nodes('a') %>%
html_attr("href")
if (length(author_link) ==0) {
author_link <- ''
}
dt <-
bylines[2] %>%
html_nodes('time') %>%
html_attr('datetime')
if (length(dt) ==0) {
dt <- ''
}
comments <-
container %>%
html_nodes('.c-entry-stat__comment-data') %>%
nth(1) %>%
html_text() %>%
trimws()
if (length(comments) != 0) {
comments <- as.numeric(comments)
comments_link <-
container %>%
html_nodes('.c-entry-stat--words') %>%
html_nodes('a') %>%
html_attr("href")
# OR
# comments_link <- paste0(article_link,'#comments')
} else {
comments <- ''
comments_link <- ''
}
c=c+1
print(c)
# it will be a one row data.frame
return(data.frame('date'=dt, 'title'=title, 'article_link'= article_link,'author'=author, 'author_link'=author_link, 'comments'=comments, 'comments_link'=comments_link))
})
scrape_page <- function(url){
p<-read_html(url)
write_html(p,'p.html')
# Get container divs
containers <-
p %>% html_nodes('.c-compact-river__entry')
# Iterate through the containers and extract html tags
list_of_df <-
lapply(containers, function(container){
title <-
container %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_text()
article_link <-
container %>%
html_nodes('.c-entry-box--compact__title') %>%
html_nodes('a') %>%
html_attr("href")
bylines <-
container %>%
html_nodes('.c-byline__item')
author <-
container %>%
html_nodes('.c-byline__author-name') %>%
html_text()
if (length(author) ==0) {
author <- ''
}
author_link <-
bylines[1] %>%
html_nodes('a') %>%
html_attr("href")
if (length(author_link) ==0) {
author_link <- ''
}
dt <-
bylines[2] %>%
html_nodes('time') %>%
html_attr('datetime')
if (length(dt) ==0) {
dt <- ''
}
comments <-
container %>%
html_nodes('.c-entry-stat__comment-data') %>%
nth(1) %>%
html_text() %>%
trimws()
if (length(comments) != 0) {
comments <- as.numeric(comments)
comments_link <-
container %>%
html_nodes('.c-entry-stat--words') %>%
html_nodes('a') %>%
html_attr("href")
# OR
# comments_link <- paste0(article_link,'#comments')
} else {
comments <- ''
comments_link <- ''
}
# it will be a one row data.frame
return(data.frame('date'=dt, 'title'=title, 'article_link'= article_link,'author'=author, 'author_link'=author_link, 'comments'=comments, 'comments_link'=comments_link))
})
df_page = rbindlist(list_of_df)
return(df_page)
}
scrape_page("https://www.theverge.com/search?page=1&q=ubuntu")
result<-scrape_data_verge("linux")
result<-scrape_data_verge("linux",5)
View(result)
View(result)
result<-scrape_data_verge("linux",6)
scrape_data_verge <- function(keywords,till_page=0){
# saving the domain name to variable
d='https://www.theverge.com'
#creating the query string from keyword(s)
#keywords=c()
kwq=paste0('q=',keywords[1])
if (length(keywords)>1){
for (kw in keywords[2:length(keywords)]) {
kwq=paste0(kwq,'+',as.character(kw))
}
}
# create url from domain and query string to get number of pages
url=paste0(d,'/search?',kwq)
pages <- get_num_pages(url)
if (till_page > 0) {
pages <- till_page
}
# Create urls for iterative scraping
urls=c()
for (page in 1:pages){
url=paste0(d,'/search?page=',page,'&',kwq)
urls <- urls %>% append(url)
}
print(urls)
df_list_urls <- lapply(urls, scrape_page)
return(rbindlist(df_list_urls))
}
result<-scrape_data_verge("linux")
result<-scrape_data_verge("ubuntu")
result<-scrape_data_verge("ubuntu")
pages_till_2<-scrape_data_verge("ubuntu",2)
write.csv(pages_till_2,"verge_ubuntu_p2.csv")
write.csv(pages_till_2,"verge_ubuntu_p2.csv",row.names = FALSE)
saveRDS(pages_till_2, file = "verge_ubuntu_p2.rds")
setwd("C:/Users/Dominik/OneDrive - Central European University/1st_trimester/C2-Webscraping/webscraping/Project1_Dominik_Gulacsy")
# Export to csv
write.csv(pages_till_2,"verge_ubuntu_p2.csv",row.names = FALSE)
saveRDS(pages_till_2, file = "verge_ubuntu_p2.rds")
1. Geting the number of pages for a given URL
```
get_num_pages<-function(url){
#get the maximum number of pages to iterate through
doc <- read_html(url)
pages <-
doc %>%
html_nodes('.c-pagination__text')
write_html(p,'p.html')
# Get container divs
containers <-
p %>% html_nodes('.c-compact-river__entry')
url
url[-1]
substrRight(url, 6)
library(stringr)
str_sub(url,-5,-1)
str_sub(url,-20,-1)
str_sub(url,-14,-1)
write_html(p,'page=3&q=linux.html')
```{r echo=FALSE}
# Test function
data<-scrape_page('https://www.theverge.com/search?page=1&q=zoom+linux')
View(data)
dp_summary_stats_pizza <- summarise(dp,
variable = "Price of Pizza",
n= n(),
mean = mean(x = price_marg),
median = median(x = price_marg),
min= min(price_marg),
max = max(price_marg),
sd = sd(price_marg),
skew = skewness(price_marg))
dp_summary_stats_bev <- summarise(dp,
variable = "Price of Beverage",
n= n(),
mean = mean(x = price_bev),
median = median(x = price_bev),
min= min(price_bev),
max = max(price_bev),
sd = sd(price_bev),
skew = skewness(price_bev))
table_summary <- add_row(dp_summary_stats_pizza,dp_summary_stats_bev)
kable(table_summary)
#install.packages('pander')
library(tidyverse)
library(geosphere)
library(moments)
library(dplyr)
library(knitr)
library(pander)
## Github Repo of Assignment: https://github.com/Julianna-szabo/DA_team_project
# Import data via URL
my_url <- "https://raw.githubusercontent.com/Julianna-szabo/DA_team_project/master/data/raw/dp.csv"
dp <- read_csv(my_url)
dp_summary_stats_pizza <- summarise(dp,
variable = "Price of Pizza",
n= n(),
mean = mean(x = price_marg),
median = median(x = price_marg),
min= min(price_marg),
max = max(price_marg),
sd = sd(price_marg),
skew = skewness(price_marg))
dp_summary_stats_bev <- summarise(dp,
variable = "Price of Beverage",
n= n(),
mean = mean(x = price_bev),
median = median(x = price_bev),
min= min(price_bev),
max = max(price_bev),
sd = sd(price_bev),
skew = skewness(price_bev))
table_summary <- add_row(dp_summary_stats_pizza,dp_summary_stats_bev)
kable(table_summary)
